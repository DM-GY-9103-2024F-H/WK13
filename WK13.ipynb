{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 13\n",
    "\n",
    "More Neural Networks for images... and CNNs..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -q https://github.com/DM-GY-9103-2024F-H/9103-utils/raw/main/src/data_utils.py\n",
    "!wget -q https://github.com/DM-GY-9103-2024F-H/9103-utils/raw/main/src/image_utils.py\n",
    "!wget -q https://github.com/DM-GY-9103-2024F-H/WK13/raw/main/WK13_utils.py\n",
    "\n",
    "!wget -qO- https://github.com/DM-GY-9103-2024F-H/9103-utils/releases/latest/download/lfw.tar.gz | tar xz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from torchvision.models import resnet34, ResNet34_Weights\n",
    "from torchvision.transforms import v2\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "from data_utils import classification_error, display_confusion_matrix\n",
    "from image_utils import make_image\n",
    "\n",
    "from WK13_utils import LFWUtils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review\n",
    "\n",
    "Let's quickly review the model from last week.\n",
    "\n",
    "### Load Data\n",
    "\n",
    "We're going to use the same `Dataset` as last week to create our `DataLoader` objects.\n",
    "\n",
    "This is a plain `Dataset` that only has to return image, label pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FaceDatasetTensor(Dataset):\n",
    "  def __init__(self, pixels, labels):\n",
    "    self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    self.pixels = pixels.to(self.device)\n",
    "    self.labels = labels.to(self.device)\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.labels)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    pixels, label = self.pixels[idx], self.labels[idx]\n",
    "    return pixels, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `train_test_split` function this week already returns `Tensor` objects for train and test data, so we just have to give those to the `Dataset` and `DataLoader` objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pixels, test_pixels, train_labels, test_labels = LFWUtils.train_test_split(0.5)\n",
    "\n",
    "train = DataLoader(FaceDatasetTensor(train_pixels, train_labels), batch_size=256, shuffle=True)\n",
    "test = DataLoader(FaceDatasetTensor(test_pixels, test_labels), batch_size=512, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look at Data\n",
    "\n",
    "Our `DataLoaders` are iterable objects, which means we need to do a bit of unpacking to get to actual labels and pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img, label = next(iter(train))\n",
    "print(LFWUtils.LABELS[label[0]])\n",
    "display(make_image(img[0], width=130))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model, Optimizer, Cost/Loss Function\n",
    "\n",
    "This is the model from last week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "image_length = LFWUtils.IMAGE_SIZE[0] * LFWUtils.IMAGE_SIZE[1]\n",
    "\n",
    "model =  nn.Sequential(\n",
    "  nn.Dropout(0.35),\n",
    "  nn.Linear(image_length, image_length // 2),\n",
    "  nn.BatchNorm1d(image_length // 2),\n",
    "  nn.ReLU(),\n",
    "  # nn.LayerNorm(image_length // 2),\n",
    "\n",
    "  nn.Dropout(0.35),\n",
    "  nn.Linear(image_length // 2, image_length // 16),\n",
    "  nn.BatchNorm1d(image_length // 16),\n",
    "  nn.ReLU(),\n",
    "  # nn.LayerNorm(image_length // 16),\n",
    "\n",
    "  nn.Dropout(0.35),\n",
    "  nn.Linear(image_length // 16, image_length // 32),\n",
    "  nn.BatchNorm1d(image_length // 32),\n",
    "  nn.ReLU(),\n",
    "  # nn.LayerNorm(image_length // 32),\n",
    "\n",
    "  nn.Dropout(0.35),\n",
    "  nn.Linear(image_length // 32, len(LFWUtils.LABELS)),\n",
    ").to(mdevice)\n",
    "\n",
    "learning_rate = 5e-3\n",
    "optim = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "img, label = next(iter(train))\n",
    "out = model(img)\n",
    "\n",
    "print(\"Input shape:\", img.shape)\n",
    "print(\"Output shape:\", out.shape)\n",
    "print(\"Parameters:\", LFWUtils.count_parameters(model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for e in range(32):\n",
    "  model.train()\n",
    "  for imgs, labels in train:\n",
    "    optim.zero_grad()\n",
    "    labels_pred = model(imgs)\n",
    "    loss = loss_fn(labels_pred, labels)\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "\n",
    "  if e % 4 == 3:\n",
    "    print(f\"Epoch: {e} loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eval\n",
    "\n",
    "Could've been in the loop, but we already know this model..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels, train_predictions = LFWUtils.get_labels(model, train)\n",
    "test_labels, test_predictions = LFWUtils.get_labels(model, test)\n",
    "train_error = classification_error(train_labels, train_predictions)\n",
    "test_error = classification_error(test_labels, test_predictions)\n",
    "print(f\"train error: {train_error:.4f}, test error: {test_error:.4f}\")\n",
    "\n",
    "display_confusion_matrix(train_labels, train_predictions, display_labels=LFWUtils.LABELS)\n",
    "display_confusion_matrix(test_labels, test_predictions, display_labels=LFWUtils.LABELS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation\n",
    "\n",
    "Our model did ok, given the limited data that it has to learn from.\n",
    "\n",
    "Adding Dropout and Batch Normalization helped, but nothing we could do about having few images for some classes...\n",
    "\n",
    "Until now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Training Harder Again\n",
    "\n",
    "One technique for improving the training of a model is to add variability to the training data. This can be done with any kind of data, but is a little bit easier to see with images.\n",
    "\n",
    "### Image augmentation\n",
    "\n",
    "This is the process of adding similar, but different data to our training dataset. The new images are created by performing shape and color transformations on the original images, to get variations of the original shapes that we are trying to detect/classify.\n",
    "\n",
    "Intuitively, what we're trying to do is give the neural network more views of the same objects, and train it to be robust to shape distortions and color variations.\n",
    "\n",
    "A sideways, distorted and green Arnold Schwarzenegger is still an Arnold Schwarzenegger.\n",
    "\n",
    "<!-- <img src=\"./imgs/augmentation.jpg\" width=\"800px\" /> -->\n",
    "<img src=\"https://i.postimg.cc/kMWgnRLK/augmentation.jpg\" width=\"800px\"/>\n",
    "\n",
    "The `PyTorch` library provides a couple of handy transformation functions for us to add to our training dataset. A more complete list and examples cna be found [HERE](https://pytorch.org/vision/0.13/auto_examples/plot_transforms.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Many Transforms with Composition\n",
    "\n",
    "Let's create a series of transforms and check what they do to the images.\n",
    "\n",
    "Some of these have a probability argument `p`, which can be used to specify how often the transformation should be performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmentations = v2.Compose([\n",
    "    v2.RandomHorizontalFlip(p=0.5),\n",
    "    v2.RandomRotation(degrees=15),\n",
    "    v2.RandomPerspective(distortion_scale=0.15, p=0.5),\n",
    "    # v2.RandomResizedCrop(size=(170, 130), scale=(.75, .9), antialias=True),\n",
    "    # v2.RandomAffine(degrees=15, translate=(0.1, 0.3), scale=(1.1, 1.5))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test on the first image of the first batch of our train dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img, label = next(iter(train))\n",
    "print(LFWUtils.LABELS[label[0]])\n",
    "img0 = make_image(img[0], width=130)\n",
    "\n",
    "print(\"Original\")\n",
    "display(img0)\n",
    "\n",
    "print(\"Transformed\")\n",
    "timg0 = augmentations(img0)\n",
    "display(timg0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Augment the DataLoader\n",
    "\n",
    "Since we have to perform this for images in the training dataset, and the `DataLoader` is responsible for giving us batches of randomly ordered images, it makes sense to delegate the transformations to the `Dataset` class that is responsible for preparing the data for the `DataLoader`.\n",
    "\n",
    "When it's time for our `Dataset` to grab an image, if there's a transformation function defined, and we're training (computing grads), we'll apply the transformation. Something like:\n",
    "\n",
    "```py\n",
    "if self.transform and torch.is_grad_enabled():\n",
    "  img = self.transform(img)\n",
    "```\n",
    "\n",
    "Before we can run the transforms we have to make sure the images have the right shape. They can't just be a list of pixels, but have to be $2D$ Tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FaceDatasetAugmentation(Dataset):\n",
    "  def __init__(self, pixels, labels, train_transform=None):\n",
    "    self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    self.pixels = pixels.to(self.device)\n",
    "    self.labels = labels.to(self.device)\n",
    "    self.train_transform = train_transform\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.labels)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    pixels, label = self.pixels[idx], self.labels[idx]\n",
    "\n",
    "    if self.train_transform and torch.is_grad_enabled():\n",
    "      #  make into 1-channel 2D image\n",
    "      pixels = pixels.view(-1, LFWUtils.IMAGE_SIZE[1], LFWUtils.IMAGE_SIZE[0])\n",
    "      # apply transforms\n",
    "      pixels = self.train_transform(pixels)\n",
    "      # turn back into list of pixels as expected by NN\n",
    "      pixels = pixels.view(-1)\n",
    "\n",
    "    return pixels, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can get our loaders one more time, but this time, with image augmentation transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pixels, test_pixels, train_labels, test_labels = LFWUtils.train_test_split(0.5)\n",
    "\n",
    "train = DataLoader(FaceDatasetAugmentation(train_pixels, train_labels, augmentations), batch_size=256, shuffle=True)\n",
    "test = DataLoader(FaceDatasetAugmentation(test_pixels, test_labels), batch_size=512, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img, label = next(iter(train))\n",
    "print(LFWUtils.LABELS[label[0]])\n",
    "img0 = make_image(img[0], width=130)\n",
    "display(img0)\n",
    "\n",
    "with torch.no_grad():\n",
    "  img, label = next(iter(test))\n",
    "  print(LFWUtils.LABELS[label[0]])\n",
    "  img0 = make_image(img[0], width=130)\n",
    "  display(img0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Re-Train\n",
    "\n",
    "Let's re-define our network, optimizer and loss function and see if this helps with training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "image_length = LFWUtils.IMAGE_SIZE[0] * LFWUtils.IMAGE_SIZE[1]\n",
    "\n",
    "model =  nn.Sequential(\n",
    "  nn.Dropout(0.35),\n",
    "  nn.Linear(image_length, image_length // 2),\n",
    "  nn.BatchNorm1d(image_length // 2),\n",
    "  nn.ReLU(),\n",
    "  # nn.LayerNorm(image_length // 2),\n",
    "\n",
    "  nn.Dropout(0.35),\n",
    "  nn.Linear(image_length // 2, image_length // 16),\n",
    "  nn.BatchNorm1d(image_length // 16),\n",
    "  nn.ReLU(),\n",
    "  # nn.LayerNorm(image_length // 16),\n",
    "\n",
    "  nn.Dropout(0.35),\n",
    "  nn.Linear(image_length // 16, image_length // 32),\n",
    "  nn.BatchNorm1d(image_length // 32),\n",
    "  nn.ReLU(),\n",
    "  # nn.LayerNorm(image_length // 32),\n",
    "\n",
    "  nn.Dropout(0.35),\n",
    "  nn.Linear(image_length // 32, len(LFWUtils.LABELS)),\n",
    ").to(mdevice)\n",
    "\n",
    "learning_rate = 5e-3\n",
    "optim = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "img, label = next(iter(train))\n",
    "out = model(img)\n",
    "\n",
    "print(\"Input shape:\", img.shape)\n",
    "print(\"Output shape:\", out.shape)\n",
    "print(\"Parameters:\", LFWUtils.count_parameters(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for e in range(32):\n",
    "  model.train()\n",
    "  for imgs, labels in train:\n",
    "    optim.zero_grad()\n",
    "    labels_pred = model(imgs)\n",
    "    loss = loss_fn(labels_pred, labels)\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "\n",
    "  if e % 4 == 3:\n",
    "    print(f\"Epoch: {e} loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels, train_predictions = LFWUtils.get_labels(model, train)\n",
    "test_labels, test_predictions = LFWUtils.get_labels(model, test)\n",
    "train_error = classification_error(train_labels, train_predictions)\n",
    "test_error = classification_error(test_labels, test_predictions)\n",
    "print(f\"train error: {train_error:.4f}, test error: {test_error:.4f}\")\n",
    "\n",
    "display_confusion_matrix(train_labels, train_predictions, display_labels=LFWUtils.LABELS)\n",
    "display_confusion_matrix(test_labels, test_predictions, display_labels=LFWUtils.LABELS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aside: Cost vs. Eval Functions\n",
    "\n",
    "#### Why we need both\n",
    "\n",
    "Cost can go down without improving accuracy, precision or recall.\n",
    "\n",
    "Image we have $2$ samples, with the following correct labels:\n",
    "\n",
    "`[0, 1]`\n",
    "\n",
    "Out model computes the following probabilities for each label of each sample:\n",
    "\n",
    "`[0.1, 0.9], [0.9, 0.1]`\n",
    "\n",
    "which means it predicts `[1, 0]` for an accuracy of $0\\%$, and a cross entropy loss of $1.7$.\n",
    "\n",
    "After some amount of training, our model improves and now gives the following probabilities for the $2$ samples:\n",
    "\n",
    "`[0.45, 0.55], [0.51, 0.49]`\n",
    "\n",
    "making the predicted labels `[1, 0]`, so accuracy is still $0\\%$, while the cross entropy loss decreased to $0.7$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spatial Information\n",
    "\n",
    "Our fully-connected layers do ok for this dataset, but they are not very efficient.\n",
    "\n",
    "There are $2$ main problems with using this approach to extract information about images:\n",
    "\n",
    "### Every pixel is connected to every other pixel\n",
    "\n",
    "Consider the first layer after the input layer: every neuron gets information about every pixel. This means that the content at the top-left corner of our image is connected to the content at the bottom-right corner, which is inefficient. We probably don't need our network to consider the entire content of the image at once in order to make decisions. It jumbles the pixel order and just makes the process harder. We might be better off telling our network to consider groups of neighboring pixels, since it's most likely for visual features to come from pixels that are near each other. In other words, we want to extract and preserve some kind of relative _Locality_ from our pixels.\n",
    "\n",
    "### Not all Arnolds are the same\n",
    "\n",
    "Let's say our network learned how to classify an Arnold Schwarzenegger that's closer to the left side of the image. If it wants to detect Arnolds on the right side of the image, or towards the top, it has to learn how to activate neurons that are associated with those sections of the image. This is also inefficient because it has to relearn to detect the same thing again, just because it's somewhere else in the image.\n",
    "\n",
    "Again, what we would like to do is group neighboring pixels, and have the groups go through similar neurons, so that any kind of learning can be applied independent of where shapes are located in the image. The technical name for this property is _Translation Invariance_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutions\n",
    "\n",
    "We could try to come up with our own architecture and write some code for a neural network that doesn't fully connect our pixels, but rather considers neighboring regions of our image in groups of neurons.\n",
    "\n",
    "But, luckily, some maths combined with intuition from old-school systems and feature engineering can help us here.\n",
    "\n",
    "There's a type of mathematical operation called a convolution that combines $2$ arbitrary functions into a new function that basically has information about all the possible combinations of inputs for the $2$ original functions. The math definition looks like this:\n",
    "\n",
    "$\\displaystyle (f * g)(\\tau) = \\int_{-\\infty}^{\\infty}{f(\\tau)}g(1 - \\tau) d\\tau$\n",
    "\n",
    "For the practical, intuitive, definition of this operation when dealing with images, we'll make $f()$ be an image and $g()$ be different, but specific, combinations of numbers organized into $2D$ matrices called kernels.\n",
    "\n",
    "When we _convolve_ the image with the kernel, we calculate every possible overlap of our kernel with the image and, depending on the numbers we choose for the kernel, can extract different types of features from our pixels.\n",
    "\n",
    "<!-- <img src=\"./imgs/kernel_slide.jpg\" height=\"256px\"/> -->\n",
    "<img src=\"https://i.postimg.cc/wMzHtFnJ/kernel-slide.jpg\" height=\"256px\"/>\n",
    "\n",
    "[SOME ANIMATIONS](https://hannibunny.github.io/mlbook/neuralnetworks/convolutionDemos.html)\n",
    "\n",
    "\n",
    "Classic image processing kernels for sharpening an image and extracting edges:\n",
    "\n",
    "<!-- <img src=\"./imgs/image_kernels.jpg\" height=\"300px\"/> -->\n",
    "<img src=\"https://i.postimg.cc/fTKWdnzb/image-kernels.jpg\" height=\"300px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The nice thing about these kernels is that they operate on neighboring pixels by default, so they already take into account the _locality_ of the features they're trying to detect.\n",
    "\n",
    "We can now set up a neural network that is a collection of $2D$ image kernels, and let our training algorithm learn parameters for these kernels based on the training data. We don't have to specify that we want an edge-detection kernel, or a curved-shape kernel, or a horizontal blur kernel... the network will learn the kernels that it needs.\n",
    "\n",
    "And, since the same kernel slides over an entire image during convolution, once the network learns to extract lines on the left side of the image, it also knows how to extract lines on the right side of the image, or on top, or anywhere else. The parameters to the kernel are the same, they just get applied to different neighborhoods of pixels.\n",
    "\n",
    "If we combine our bank of kernels with another operation to reduce the size of our image as it moves through the network, we can create a type of dynamic filtering that detects whether certain features are present on our image.\n",
    "\n",
    "<!-- <img src=\"./imgs/cnn_layers.jpg\" height=\"320px\"/> -->\n",
    "<img src=\"https://i.postimg.cc/rpdq7DSd/cnn-layers.jpg\" height=\"320px\"/>\n",
    "\n",
    "Then, after we have reduced our feature maps to a small-enough shape we ca use fully-connected layers to finalize our classification.\n",
    "\n",
    "<!-- <img src=\"./imgs/cnn_fc.jpg\" height=\"320px\"/> -->\n",
    "<img src=\"https://i.postimg.cc/XYYjcHqk/cnn-fc.jpg\" height=\"320px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New Network, New Loader\n",
    "\n",
    "Right now our loader returns lists of pixels for each image in a `tensor` of shape $(1, 22100)$. In order to use the convolution layers we actually need the images in a $2D$ tensor of shape $(170, 130)$.\n",
    "\n",
    "If we were working with color images, CNNs are even more particular, and instead of working with the original image shape of $H \\times W \\times C$ (`height` by `width` by `channel`), it wants a `tensor` with shape $C \\times W \\times H$.\n",
    "\n",
    "It also doesn't like pixel values being in the range $[0 - 255]$, but wants them to be between $[0 - 1]$.\n",
    "\n",
    "Our previous `Dataset` had code to turn pixel lists into $2D$ tensors, but only for data that had to go through augmentation.\n",
    "\n",
    "Let's define a more generic `Dataset` that returns tensors with the right shapes and right ranges for our `CNN`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmentations = v2.Compose([\n",
    "\tv2.RandomHorizontalFlip(p=0.5),\n",
    "\tv2.RandomRotation(degrees=15),\n",
    "\tv2.RandomPerspective(distortion_scale=0.15, p=0.5),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FaceDatasetCNN(Dataset):\n",
    "  def __init__(self, pixels, labels, train_transform=None):\n",
    "    self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    self.pixels = pixels.to(self.device)\n",
    "    self.labels = labels.to(self.device)\n",
    "    self.train_transform = train_transform\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.labels)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    pixels, label = self.pixels[idx], self.labels[idx]\n",
    "\n",
    "    #  make into 1-channel 2D image\n",
    "    pixels = pixels.view(-1, LFWUtils.IMAGE_SIZE[1], LFWUtils.IMAGE_SIZE[0])\n",
    "    # normalize to [0-1]\n",
    "    pixels /= pixels.max()\n",
    "\n",
    "    if self.train_transform and torch.is_grad_enabled():\n",
    "      pixels = self.train_transform(pixels)\n",
    "\n",
    "    return pixels, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pixels, test_pixels, train_labels, test_labels = LFWUtils.train_test_split(0.5)\n",
    "\n",
    "train = DataLoader(FaceDatasetCNN(train_pixels, train_labels, augmentations), batch_size=256, shuffle=True)\n",
    "test = DataLoader(FaceDatasetCNN(test_pixels, test_labels), batch_size=512, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check if we're getting something sensible and if the transformations are working."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img, label = next(iter(train))\n",
    "\n",
    "print(\"batch shape: B x C x H x W\")\n",
    "display(img.shape)\n",
    "\n",
    "print(\"img shape: C x H x W\")\n",
    "display(img[0].shape)\n",
    "\n",
    "img0 = make_image(255*img[0], width=130)\n",
    "display(img0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define CNN Model\n",
    "\n",
    "This is how we define a convolution layer:\n",
    "\n",
    "`nn.Conv2d(Cin, Cout, kernel_size)`\n",
    "\n",
    "Where `Cin` is the number of input channels, `Cout` is output channels, and `kernel_size` the width of our kernel.\n",
    "\n",
    "We should still normalize the computations by batch, but this time using the $2D$ version of `BatchNorm()`, and after activation we perform the `MaxPool` operation, which takes the largest value in a $2 \\times 2$ region of our activations and condenses them into denser representation of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "width, height = LFWUtils.IMAGE_SIZE\n",
    "\n",
    "pool_width = (width - 2) // 2\n",
    "pool_height = (height - 2) // 2\n",
    "\n",
    "linear_length = pool_width * pool_height * 32\n",
    "\n",
    "model = nn.Sequential(\n",
    "  nn.Conv2d(1, 32, 3),\n",
    "  nn.BatchNorm2d(32),\n",
    "  nn.ReLU(),\n",
    "  nn.MaxPool2d(2),\n",
    "\n",
    "  # More Convs ?\n",
    "\n",
    "  nn.Flatten(),\n",
    "\n",
    "  nn.Dropout(0.35),\n",
    "  nn.Linear(linear_length, 512),\n",
    "  nn.ReLU(),\n",
    "\n",
    "  nn.Dropout(0.35),\n",
    "  nn.Linear(512, len(LFWUtils.LABELS)),\n",
    ").to(mdevice)\n",
    "\n",
    "learning_rate = 5e-3\n",
    "optim = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "img, label = next(iter(train))\n",
    "out = model(img)\n",
    "\n",
    "print(\"Input shape:\", img.shape)\n",
    "print(\"Output shape:\", out.shape)\n",
    "print(\"Parameters:\", LFWUtils.count_parameters(model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for e in range(32):\n",
    "  model.train()\n",
    "  for imgs, labels in train:\n",
    "    optim.zero_grad()\n",
    "    labels_pred = model(imgs)\n",
    "    loss = loss_fn(labels_pred, labels)\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "\n",
    "  if e % 4 == 3:\n",
    "    print(f\"Epoch: {e} loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels, train_predictions = LFWUtils.get_labels(model, train)\n",
    "test_labels, test_predictions = LFWUtils.get_labels(model, test)\n",
    "train_error = classification_error(train_labels, train_predictions)\n",
    "test_error = classification_error(test_labels, test_predictions)\n",
    "print(f\"train error: {train_error:.4f}, test error: {test_error:.4f}\")\n",
    "\n",
    "display_confusion_matrix(train_labels, train_predictions, display_labels=LFWUtils.LABELS)\n",
    "display_confusion_matrix(test_labels, test_predictions, display_labels=LFWUtils.LABELS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer Learning\n",
    "\n",
    "The CNN architecture is so stable that models can be made to be very deep, some with $100\\text{s}$ of layers.\n",
    "\n",
    "The internal layers of these models are so abstract and generic that once a model has been trained on millions of data samples (images), it learns and retains information not only about the images on the dataset, but any visual pattern that it learned in the process.\n",
    "\n",
    "It's not uncommon to use a previously trained model for a similar-but-different project, even if the images have nothing in common. Generic information about images can be transferred to new datasets and problem spaces.\n",
    "\n",
    "<!-- <img src=\"./imgs/resnet_activation_00.jpg\" height=\"300px\" /> -->\n",
    "<img src=\"https://i.postimg.cc/tR3twzmz/resnet-activation-00.jpg\" height=\"300px\" />\n",
    "\n",
    "<!-- <img src=\"./imgs/resnet_activation_01.jpg\" height=\"300px\" /> -->\n",
    "<img src=\"https://i.postimg.cc/hPKbB7kR/resnet-activation-01.jpg\" height=\"300px\" />\n",
    "\n",
    "<!-- <img src=\"./imgs/resnet_activation_02.jpg\" height=\"300px\" /> -->\n",
    "<img src=\"https://i.postimg.cc/15M053Sn/resnet-activation-03.jpg\" height=\"300px\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Residual Networks\n",
    "\n",
    "There are a couple of families of CNN networks that get used as the starting point for many different types of visual models (and also audio and text). One such architecture is [ResNet](https://arxiv.org/abs/1512.03385).\n",
    "\n",
    "ResNet comes in a few sizes/depths, and PyTorch has at least [5 pre-trained ResNet models](https://pytorch.org/hub/pytorch_vision_resnet/) that we can use.\n",
    "\n",
    "These PyTorch ResNet models were trained on the [ImageNet](https://image-net.org/download.php) dataset. This dataset has $1\\text{,}281\\text{,}167$ training images and classifies objects into $1\\text{,}000$ classes.\n",
    "\n",
    "We'll use the `ReNet34` model, which is not the largest, but will fit nicely into small GPUs.\n",
    "\n",
    "<!-- <img src=\"./imgs/resnet34_00.jpg\" width=\"900px\" /> -->\n",
    "<img src=\"https://i.postimg.cc/XNc8xdqy/resnet34-00.jpg\" width=\"900px\" />\n",
    "\n",
    "<!-- <img src=\"./imgs/resnet34_01.jpg\" width=\"900px\" /> -->\n",
    "<img src=\"https://i.postimg.cc/hP20Rn9D/resnet34-01.jpg\" width=\"900px\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiating ResNet\n",
    "\n",
    "Is easy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = resnet34(weights=ResNet34_Weights.DEFAULT)\n",
    "display(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adjust inputs\n",
    "\n",
    "From https://pytorch.org/hub/pytorch_vision_resnet/:\n",
    "\n",
    "_All pre-trained models expect input images normalized in the same way, i.e. mini-batches of 3-channel RGB images of shape (3 x H x W), where H and W are expected to be at least 224. The images have to be loaded in to a range of [0, 1] and then normalized using mean = [0.485, 0.456, 0.406] and std = [0.229, 0.224, 0.225]._\n",
    "\n",
    "We can use `PyTorch` transformation functions to achieve this, but this means that now we'll have some transformations that always have to happen and some that only happen in the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transforms = v2.Compose([\n",
    "\tv2.RandomHorizontalFlip(p=0.5),\n",
    "\tv2.RandomRotation(degrees=15),\n",
    "\tv2.RandomPerspective(distortion_scale=0.15, p=0.5),\n",
    "])\n",
    "\n",
    "eval_transforms = v2.Compose([\n",
    "  v2.Resize(224),\n",
    "  v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to change our `Dataset` accordingly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FaceDatasetResNet(Dataset):\n",
    "  def __init__(self, pixels, labels, train_transform=None, eval_transform=None):\n",
    "    self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    self.pixels = pixels.to(self.device)\n",
    "    self.labels = labels.to(self.device)\n",
    "    self.train_transform = train_transform\n",
    "    self.eval_transform = eval_transform\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.labels)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    pixels, label = self.pixels[idx], self.labels[idx]\n",
    "\n",
    "    #  make into 1-channel 2D image\n",
    "    pixels = pixels.view(-1, LFWUtils.IMAGE_SIZE[1], LFWUtils.IMAGE_SIZE[0])\n",
    "    # normalize to [0-1]\n",
    "    pixels /= pixels.max()\n",
    "\n",
    "    if self.train_transform and torch.is_grad_enabled():\n",
    "      pixels = self.train_transform(pixels)\n",
    "    \n",
    "    # ResNet expects color images, so we have to replicate our single channel\n",
    "    pixels = torch.stack([pixels[0],pixels[0],pixels[0]], dim=0)\n",
    "\n",
    "    # ResNet specific transformations\n",
    "    if self.eval_transform:\n",
    "      pixels = self.eval_transform(pixels)\n",
    "\n",
    "    return pixels, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pixels, test_pixels, train_labels, test_labels = LFWUtils.train_test_split(0.5)\n",
    "\n",
    "train = DataLoader(FaceDatasetResNet(train_pixels, train_labels, train_transforms, eval_transforms), batch_size=256, shuffle=True)\n",
    "test = DataLoader(FaceDatasetResNet(test_pixels, test_labels, eval_transform=eval_transforms), batch_size=512, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img,label = next(iter(train))\n",
    "print(img.shape, img[0].shape)\n",
    "display(v2.ToPILImage()(img[0]))\n",
    "\n",
    "with torch.no_grad():\n",
    "  img,label = next(iter(test))\n",
    "  print(img.shape, img[0].shape)\n",
    "  display(v2.ToPILImage()(img[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model.fc = nn.Linear(model.fc.in_features, len(LFWUtils.LABELS))\n",
    "model = model.to(mdevice)\n",
    "\n",
    "learning_rate = 5e-3\n",
    "optim = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "img, label = next(iter(train))\n",
    "out = model(img)\n",
    "\n",
    "print(\"Input shape:\", img.shape)\n",
    "print(\"Output shape:\", out.shape)\n",
    "print(\"Parameters:\", LFWUtils.count_parameters(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for e in range(16):\n",
    "  model.train()\n",
    "  for imgs, labels in train:\n",
    "    optim.zero_grad()\n",
    "    labels_pred = model(imgs)\n",
    "    loss = loss_fn(labels_pred, labels)\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "\n",
    "  if e % 4 == 3:\n",
    "    print(f\"Epoch: {e} loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels, train_predictions = LFWUtils.get_labels(model, train)\n",
    "test_labels, test_predictions = LFWUtils.get_labels(model, test)\n",
    "train_error = classification_error(train_labels, train_predictions)\n",
    "test_error = classification_error(test_labels, test_predictions)\n",
    "print(f\"train error: {train_error:.4f}, test error: {test_error:.4f}\")\n",
    "\n",
    "display_confusion_matrix(train_labels, train_predictions, display_labels=LFWUtils.LABELS)\n",
    "display_confusion_matrix(test_labels, test_predictions, display_labels=LFWUtils.LABELS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Layers\n",
    "\n",
    "That worked really well. The information learned by the `ResNet` network on 1 million images seems to transfer to our classification of faces and we can leverage its pattern-recognition layers to build a more accurate model in a short amount of time.\n",
    "\n",
    "Let's take a look at some of the filtered images in our `ResNet` model. We can do this with an untrained model, but it's better to look at one that has been recently trained.\n",
    "\n",
    "When we displayed the model layers above we saw that the model has $4$ main groups of convolution layers. The further down the model we go, the smaller the images are, and the more abstract the activation patterns will be.\n",
    "\n",
    "At the very last layer we might have $512$ _images_ that are only $4 \\times 4$ pixels, but light-up under very specific conditions, like: is there a bird in the image ? is there a face ?\n",
    "\n",
    "To see slightly larger images we'll look at some activations on layers $1$ and $2$.\n",
    "\n",
    "We'll use the `hook` mechanism from `PyTorch` to add some auxiliary logic to the layers we are interested in looking at. This allows us to run some extra code on the layers inputs and outputs every time it processes an image.\n",
    "\n",
    "Our `hook` function will just save the layers input and output tensors to external dictionaries that we cn visualize later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activations_in = {}\n",
    "activations_out = {}\n",
    "layer_kernels = {}\n",
    "\n",
    "def get_activation(name):\n",
    "  def hook(model, input, output):\n",
    "    if name not in layer_kernels:\n",
    "      layer_kernels[name] = model.weight.detach()\n",
    "    activations_in[name] = input[0].detach()\n",
    "    activations_out[name] = output.detach()\n",
    "  return hook\n",
    "\n",
    "model.conv1.register_forward_hook(get_activation('conv1'))\n",
    "model.layer1[1].conv2.register_forward_hook(get_activation('layer1.1.conv2'))\n",
    "model.layer1[2].conv2.register_forward_hook(get_activation('layer1.2.conv2'))\n",
    "model.layer2[0].conv2.register_forward_hook(get_activation('layer2.0.conv2'))\n",
    "model = model.to(mdevice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have our `hook` in place we have to pass some data through the network so it saves the inputs and outputs for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "  img, label = next(iter(train))\n",
    "  model(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can display activations for specific images in the processed batch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_idx = 0\n",
    "channel_idx = 0\n",
    "\n",
    "img_t = img[img_idx, channel_idx]\n",
    "img_p = make_image(255 * LFWUtils.map01(img_t), width=img.shape[3])\n",
    "\n",
    "display(img_p.resize(LFWUtils.IMAGE_SIZE))\n",
    "LFWUtils.display_activation_grids(activations_out, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_idx = 1\n",
    "channel_idx = 0\n",
    "\n",
    "img_t = img[img_idx, channel_idx]\n",
    "img_p = make_image(255 * LFWUtils.map01(img_t), width=img.shape[3])\n",
    "\n",
    "display(img_p.resize(LFWUtils.IMAGE_SIZE))\n",
    "LFWUtils.display_activation_grids(activations_out, img_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can also visualize the kernel values in each layer.\n",
    "\n",
    "These are tiny $3 \\times 3$ or $7 \\times 7$ convolution kernels that get multiplied to filter the images.\n",
    "\n",
    "Other than the first one, where the kernels actually acted on 3-channel layers, the colors on the other kernels is artificial. They're the result of combining the kernels into groups of $3$, but in reality they are 64-channel kernels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LFWUtils.display_kernel_grids(layer_kernels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gradio",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
